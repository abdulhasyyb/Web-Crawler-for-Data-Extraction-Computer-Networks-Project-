# Web Crawler for Data Extraction
## Computer Networks Academic Project

---

## Overview
This project was developed as part of the **Computer Networks** course and focuses on building a **modular Python-based web crawler** to understand **HTTP-based client–server communication** and **automated data extraction** from websites.

The crawler starts from a **root URL**, navigates internal links within a **controlled depth**, extracts relevant content, and stores the data in a **structured JSON format** for analysis.

---

## Key Features
- **Automated website crawling** starting from a root URL  
- **Extraction of page titles, textual content, and image URLs**  
- **Randomized User-Agent headers** to mimic browser behavior  
- **Structured data storage** using JSON  
- **Modular and configurable design**

---

## Technologies Used
- **Python 3**
- **Requests**
- **BeautifulSoup (bs4)**
- **Fake-UserAgent**
- **JSON**

---

## Networking Concepts Applied
- **HTTP request–response model**
- **Client–server communication**
- **User-Agent handling**
- **Controlled crawling depth**

---

## Project Structure

Web-Crawler-Data-Extraction
│
├── src/
│ ├── main.py
│ ├── crawler.py
│ ├── parser.py
│ ├── storage.py
│ ├── utils.py
│ └── config.py
│
├── report/
│ └── Project-report.pdf
│
├── sample-output/
│ └── output.json
│
└── requirements.txt



---

## Installation & Usage
```bash
pip install -r requirements.txt
python main.py
## Project Structure

Academic research and learning

Content aggregation

Market and competitor analysis

SEO and website structure analysis
